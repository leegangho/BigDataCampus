con<-tst[grep("기사 내용",tst)[1]:grep("기사 내용",tst)[2]]
}
else
{
con<-tst[grep("[(가-힣ㄱ-ㅎㅏ-ㅣ)]",tst)[ grep("[(가-힣ㄱ-ㅎㅏ-ㅣ)]",tst)>grep("기사 내용",tst)][1]]
}
# end if
cate<-"스포츠"
title<-gsub('<meta property=\"og:title\"       content=\"',"",title)
title<-gsub('\"/>',"",title)
author<-gsub('<meta property=\"og:article:author\" content=\"',"",author)
author<-gsub('\"/>',"",author)
author<-gsub("네이버 스포츠 ","",author)
author<-gsub("[[:punct:]]", "", author)
author<-gsub(" ", "", author)
postTime<-gsub("<(/)?([a-zA-Z]*)(\\s[a-zA-Z]*=[^>]*)?(\\s)*(/)?>","",postTime)
postTime<-gsub(" ","",postTime)
postTime<-paste0(substr(postTime,1,10)," ",substr(postTime,11,15))
if (length(chgTime)!=0)
{
chgTime<-gsub("\t","",chgTime)
chgTime<-gsub("<(/)?([a-zA-Z]*)(\\s[a-zA-Z]*=[^>]*)?(\\s)*(/)?>","",chgTime)
chgTime<-gsub("[[:punct:]]", "", chgTime)
chgTime<-gsub("최종수정", "", chgTime)
chgTime<-gsub(" ", "", chgTime)
chgTime<-paste0(substr(chgTime,1,4),"-",substr(chgTime,5,6),"-",substr(chgTime,7,8)," ",substr(chgTime,9,10),":",substr(chgTime,11,12))
}
else
{
chgTime<-postTime
}
# end if
con<-gsub("\t","",con)
con<-gsub("<!--(([^-]*)|([-]{1})|([-]{2})([^>]{1}))*?-->","",con)
con<-gsub("<(/)?([a-zA-Z]*)(\\s[a-zA-Z]*=[^>]*)?(\\s)*(/)?>","",con)
con<-paste(con, collapse = "")
options(warn=-1)
listTem<-tryCatch(data.frame(category=cate,title=title,author=author,postTime=postTime,chgTime=chgTime,contents=con),  error = function(e) print("Read error."))
if(!grepl("Read error",listTem))
{
dataAll<-rbind(dataAll,listTem)
print(paste(cate,j,j/dd*100,"%"))
}
else
{
print(paste("This link is not right.",tt))
print(paste(cate,j,j/dd*100,"% above"))
errorURL<<-c(errorURL,tt)
}
# end if
options(warn=1)
Sys.sleep(CTime)
}
# end if
}
# end for
}
# end if
if(selectEntertain==1)
{
dd<-length(list08)
for(j in 1:dd)
{
tt<-as.character(list08[j])
if(nchar(tt)==144|nchar(tt)==138)
{
substr(tt,1,108)->tt
}
# end if
ncnt<-1
options(warn=-1)
tst<-tryCatch(readLines(tt,warn=F,encoding="UTF-8"),  error = function(e) print("Read error, Please wait. It will be start after 1 sec."))
if(grepl("Read error",tst))
{
while(ncnt<3)
{
Sys.sleep(1)
tst<-tryCatch(readLines(tt,warn=F,encoding="UTF-8"),  error = function(e) print("Read error, Please wait. It will be start after 1 sec."))
ncnt<-ncnt+1
}
# end while
}
# end if
options(warn=1)
charSet<-length(grep("<meta charset=",tst))
if(charSet!=0)
{
options(warn=-1)
tst<-tryCatch(readLines(tt,warn=F,encoding="EUC-KR"),  error = function(e) print("Read error, Please wait. It will be start after 1 sec."))
if(grepl("Read error",tst))
{
while(ncnt<3)
{
Sys.sleep(1)
tst<-tryCatch(readLines(tt,warn=F,encoding="EUC-KR"),  error = function(e) print("Read error, Please wait. It will be start after 1 sec."))
ncnt<-ncnt+1
}
# end while
}
# end if
options(warn=1)
}
else
{
if(ncnt==3)
{
errorURL<<-c(errorURL,tt)
}
else
{
tst<-gsub('&nbsp;',"",tst)
tst<-gsub('&lt;',"<",tst)
tst<-gsub('&gt;',">",tst)
tst<-gsub('&amp;',"&",tst)
tst<-gsub('&quot;','"',tst)
tst<-gsub('&#039;',"'",tst)
tst<-gsub('&#034','"',tst)
cate<-"TV연애"
title<-tst[grep("og:title",tst)]
author<-tst[grep(":author",tst)]
postTime<-tst[grep("기사입력",tst)]
chgTime<-tst[grep("최종수정",tst)]
if(length(grep("본문 내용",tst))>1)
{
con<-tst[grep("본문 내용",tst)[1]:grep("본문 내용",tst)[2]]
}
else
{
con<-tst[grep("[(가-힣ㄱ-ㅎㅏ-ㅣ)]",tst)[ grep("[(가-힣ㄱ-ㅎㅏ-ㅣ)]",tst)>grep("본문 내용",tst)][1]]
}
# end if
cate<-gsub("[^(가-힣ㄱ-ㅎㅏ-ㅣ)]","",cate)
title<-gsub('<meta property=\"og:title\"\t\t\tcontent=\"',"",title)
title<-gsub('\"/>',"",title)
author<-gsub('<meta property=\"og:article:author\"\tcontent=\"',"",author)
author<-gsub('\"/>',"",author)
author<-gsub("네이버TV연예","",author)
author<-gsub("[[:punct:]]", "", author)
author<-gsub(" ", "", author)
postTime<-gsub("\t","",postTime)
postTime<-gsub("<(/)?([a-zA-Z]*)(\\s[a-zA-Z]*=[^>]*)?(\\s)*(/)?>","",postTime)
postTime<-gsub("기사입력","",postTime)
if (length(chgTime)!=0)
{
chgTime<-gsub("\t","",chgTime)
chgTime<-gsub("<(/)?([a-zA-Z]*)(\\s[a-zA-Z]*=[^>]*)?(\\s)*(/)?>","",chgTime)
chgTime<-gsub("최종수정","",chgTime)
}
else
{
chgTime<-postTime
}
# end if
con<-gsub("\t","",con)
con<-gsub("<!--(([^-]*)|([-]{1})|([-]{2})([^>]{1}))*?-->","",con)
con<-gsub("<(/)?([a-zA-Z]*)(\\s[a-zA-Z]*=[^>]*)?(\\s)*(/)?>","",con)
if(length(grep("script",con))==2)
{
con<-con[c(-grep("script",con)[1]:-grep("script",con)[2])]
}
# end if
con<-paste(con, collapse = "")
con<-gsub("<!--.*-->","",con)
options(warn=-1)
listTem<-tryCatch(data.frame(category=cate,title=title,author=author,postTime=postTime,chgTime=chgTime,contents=con),  error = function(e) print("Read error."))
if(!grepl("Read error",listTem))
{
dataAll<-rbind(dataAll,listTem)
print(paste(cate,j,j/dd*100,"%"))
}
else
{
print(paste("This link is not right.",tt))
print(paste(cate,j,j/dd*100,"% above"))
errorURL<<-c(errorURL,tt)
}
# end if
options(warn=1)
Sys.sleep(CTime)
}
# end if
}
# end if
}
# end for
}
# end if
if(selectWeather==1)
{
dd<-length(list09)
for(j in 1:dd)
{
tt<-as.character(list09[j])
ncnt<-1
options(warn=-1)
tst<-tryCatch(readLines(tt,warn=F,encoding="UTF-8"),  error = function(e) print("Read error, Please wait. It will be start after 1 sec."))
if(grepl("Read error",tst))
{
while(ncnt<3)
{
Sys.sleep(1)
tst<-tryCatch(readLines(tt,warn=F,encoding="UTF-8"),  error = function(e) print("Read error, Please wait. It will be start after 1 sec."))
ncnt<-ncnt+1
}
# end while
}
# end if
options(warn=1)
charSet<-length(grep("<meta charset=",tst))
if(charSet!=0)
{
options(warn=-1)
tst<-tryCatch(readLines(tt,warn=F,encoding="EUC-KR"),  error = function(e) print("Read error, Please wait. It will be start after 1 sec."))
if(grepl("Read error",tst))
{
while(ncnt<3)
{
Sys.sleep(1)
tst<-tryCatch(readLines(tt,warn=F,encoding="EUC-KR"),  error = function(e) print("Read error, Please wait. It will be start after 1 sec."))
ncnt<-ncnt+1
}
# end while
}
# end if
options(warn=1)
}
else
{
if(ncnt==3)
{
errorURL<<-c(errorURL,tt)
}
else
{
tst<-gsub('&nbsp;',"",tst)
tst<-gsub('&lt;',"<",tst)
tst<-gsub('&gt;',">",tst)
tst<-gsub('&amp;',"&",tst)
tst<-gsub('&quot;','"',tst)
tst<-gsub('&#039;',"'",tst)
tst<-gsub('&#034','"',tst)
cate<-tst[grep("<title>",tst)]
title<-tst[grep('<h4 class="new_end_tit">',tst)]
author<-tst[grep('<dd class="date">',tst)]
postTime<-tst[grep('<dd class="date">',tst)]
con<-tst[grep('<div class="data">',tst)+1]
cate<-gsub("[^(가-힣ㄱ-ㅎㅏ-ㅣ)]","",cate)
cate<-gsub(" ","",cate)
cate<-gsub("네이버","",cate)
title<-gsub('\t',"",title)
title<-gsub("<[^>]*>","",title)
author<-gsub("\t","",author)
author<-gsub('<[^>]*>',"",author)
author<-gsub("\\[","",author)
author<-gsub(" ]","",author)
author<-gsub("(19|20)..-(0[1-9]|1[012])-(0[1-9]|[12][0-9]|3[0-1]) (0[0-9]|1[0-9]|2[01234]):(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]):(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9])","",author)
author<-gsub(" ","",author)
postTime<-unlist(strsplit(postTime, ' '))
postTime<-paste(postTime[grep("(19|20)..-(0[1-9]|1[012])-(0[1-9]|[12][0-9]|3[0-1])",postTime)],postTime[grep("(0[0-9]|1[0-9]|2[01234]):(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]):(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9])",postTime)])
chgTime<-postTime
con<-gsub("\t","",con)
con<-gsub("<ul>.*</ul>","",con)
con<-gsub("<h3>.*</h3>","",con)
con<-gsub("<(/)?([a-zA-Z]*)(\\s[a-zA-Z]*=[^>]*)?(\\s)*(/)?>","",con)
options(warn=-1)
listTem<-tryCatch(data.frame(category=cate,title=title,author=author,postTime=postTime,chgTime=chgTime,contents=con),  error = function(e) print("Read error."))
if(!grepl("Read error",listTem))
{
dataAll<-rbind(dataAll,listTem)
print(paste(cate,j,j/dd*100,"%"))
}
else
{
print(paste("This link is not right.",tt))
print(paste(cate,j,j/dd*100,"% above"))
errorURL<<-c(errorURL,tt)
}
# end if
options(warn=1)
Sys.sleep(CTime)
}
# end if
}
# end for
}
}
# end if
dataAll<-dataAll[-1,]
write.csv(dataAll,"dataAll.csv",row.names=F)
print(paste("Get contents is Done! It is saved at",getwd(),"named dataAll.csv."))
}
else
{
print("No url collected.")
}
}
getNaverNews()
install.packages("rvest")
url_base<-"https://movie.daum.net/moviedb/grade?movieId=119835&type=netizen&page=2"
library(rvest)
url_base<-"https://movie.daum.net/moviedb/grade?movieId=119835&type=netizen&page=2"
for(page in 1:2){
url<-paste(url_base,page,sep='')
htxt<-read_html(url)
table<-html_nodes(table,'.desc_review')
reviews<-html_text(content)
if(length(reviews)==0){break}
reviews<-gsub("\r","",reviews)
reviews<-gsub("\t","",reviews)
all.reviews<-c(all.reviews,reviews)
print(page)
Sys.sleep(0.7)
}
library(rvest)
url_base<-"https://movie.daum.net/moviedb/grade?movieId=119835&type=netizen&page=2"
for(page in 1:2){
url<-paste(url_base,page,sep='')
htxt<-read_html(url)
table<-html_nodes(table,'.desc_review')
reviews<-html_text(content)
if(length(reviews)==0){break}
reviews<-gsub("\r","",reviews)
reviews<-gsub("\t","",reviews)
all.reviews<-c(all.reviews,reviews)
print(page)
Sys.sleep(0.7)
}
table<-html_nodes(table,'.desc_review')
url<-paste(url_base,page,sep='')
htxt<-read_html(url)
table<-html_nodes(table,'.desc_review')
library(rvest)
url_base<-"https://movie.daum.net/moviedb/grade?movieId=119835&type=netizen&page=2"
for(page in 1:2){
url<-paste(url_base,page,sep='')
htxt<-read_html(url)
table<-html_nodes(htxt,'.review_info')
content<-html_nodex(table,'.desc_review')
reviews<-html_text(content)
if(length(reviews)==0){break}
reviews<-gsub("\r","",reviews)
reviews<-gsub("\t","",reviews)
all.reviews<-c(all.reviews,reviews)
print(page)
Sys.sleep(0.7)
}
for(page in 1:2){
url<-paste(url_base,page,sep='')
htxt<-read_html(url)
table<-html_nodes(htxt,'.review_info')
content<-html_nodes(table,'.desc_review')
reviews<-html_text(content)
if(length(reviews)==0){break}
reviews<-gsub("\r","",reviews)
reviews<-gsub("\t","",reviews)
all.reviews<-c(all.reviews,reviews)
print(page)
Sys.sleep(0.7)
}
all.reviews<-c()
url_base<-"https://movie.daum.net/moviedb/grade?movieId=119835&type=netizen&page=2"
for(page in 1:2){
url<-paste(url_base,page,sep='')
htxt<-read_html(url)
table<-html_nodes(htxt,'.review_info')
content<-html_nodes(table,'.desc_review')
reviews<-html_text(content)
if(length(reviews)==0){break}
reviews<-gsub("\r","",reviews)
reviews<-gsub("\t","",reviews)
all.reviews<-c(all.reviews,reviews)
print(page)
Sys.sleep(0.7)
}
write.table(all.reviews,'daum_movie.txt')
view('daum_movie.txt')
View('daum_movie.txt')
View(all.reviews)
for(page in 1:10){
url<-paste(url_base,page,sep='')
htxt<-read_html(url)
table<-html_nodes(htxt,'.review_info')
content<-html_nodes(table,'.desc_review')
reviews<-html_text(content)
if(length(reviews)==0){break}
reviews<-gsub("\r","",reviews)
reviews<-gsub("\t","",reviews)
all.reviews<-c(all.reviews,reviews)
print(page)
Sys.sleep(0.7)
}
View(all.reviews)
all.reviews<-c()
url_base<-"https://movie.daum.net/moviedb/grade?movieId=119835&type=netizen&page=2"
for(page in 1:10){
url<-paste(url_base,page,sep='')
htxt<-read_html(url)
table<-html_nodes(htxt,'.review_info')
content<-html_nodes(table,'.desc_review')
reviews<-html_text(content)
if(length(reviews)==0){break}
reviews<-gsub("\r","",reviews)
reviews<-gsub("\t","",reviews)
all.reviews<-c(all.reviews,reviews)
print(page)
Sys.sleep(0.7)
}
View(all.reviews)
#url_base<-"https://movie.daum.net/moviedb/grade?movieId=119835&type=netizen&page=2"
url_base<-"https://movie.naver.com/movie/point/af/list.nhn?&page=2"
all.reviews<-c()
#url_base<-"https://movie.daum.net/moviedb/grade?movieId=119835&type=netizen&page=2"
url_base<-"https://movie.naver.com/movie/point/af/list.nhn?&page=2"
for(page in 1:10){
url<-paste(url_base,page,sep='')
htxt<-read_html(url)
table<-html_nodes(htxt,'.list_netizen')
content<-html_nodes(table,'.title')
reviews<-html_text(content)
if(length(reviews)==0){break}
reviews<-gsub("\r","",reviews)
reviews<-gsub("\t","",reviews)
all.reviews<-c(all.reviews,reviews)
print(page)
Sys.sleep(0.7)
}
for(page in 1:10){
url<-paste(url_base,page,sep='')
htxt<-read_html(url)
table<-html_nodes(htxt,'list_netizen')
content<-html_nodes(table,'.title')
reviews<-html_text(content)
if(length(reviews)==0){break}
reviews<-gsub("\r","",reviews)
reviews<-gsub("\t","",reviews)
all.reviews<-c(all.reviews,reviews)
print(page)
Sys.sleep(0.7)
}
table<-html_nodes(htxt,'.list_netizen')
content<-html_nodes(table,'.title')
reviews<-html_text(content)
if(length(reviews)==0){break}
reviews<-gsub("\r","",reviews)
reviews<-gsub("\t","",reviews)
all.reviews<-c(all.reviews,reviews)
print(page)
for(page in 1:10){
url<-paste(url_base,page,sep='')
htxt<-read_html(url)
table<-html_nodes(htxt,'.list_netizen')
content<-html_nodes(table,'.title')
reviews<-html_text(content)
if(length(reviews)==0){break}
reviews<-gsub("\r","",reviews)
reviews<-gsub("\t","",reviews)
all.reviews<-c(all.reviews,reviews)
print(page)
Sys.sleep(0.7)
}
all.reviews<-c()
#url_base<-"https://movie.daum.net/moviedb/grade?movieId=119835&type=netizen&page=2"
url_base<-"https://movie.naver.com/movie/point/af/list.nhn?&page=1"
for(page in 1:2){
url<-paste(url_base,page,sep='')
htxt<-read_html(url)
table<-html_nodes(htxt,'.list_netizen')
content<-html_nodes(table,'.title')
reviews<-html_text(content)
if(length(reviews)==0){break}
reviews<-gsub("\r","",reviews)
reviews<-gsub("\t","",reviews)
all.reviews<-c(all.reviews,reviews)
print(page)
Sys.sleep(0.7)
}
write.table(all.reviews,'naver_movie.txt')
View(all.reviews)
if(!require(RCurl)){install.packages("RCurl"); library(RCurl)}
install.packages("RCurl")
library(RCurl)
install.packages("RCurl")
url<-http://news.chosun.com/site/data/html dir/2018/05/23/2018052300694.html
url<-http:\\news.chosun.com\site\data\html dir\2018\05\23\2018052300694.html
url<-http://news.chosun.com/site/data/html dir/2018/05/23/2018052300694.html
url<-http://news.chosun.com/site/data/htmldir/2018/05/23/2018052300694.html
url<-http:://news.chosun.com/site/data/html dir/2018/05/23/2018052300694.html
url<-https://news.chosun.com/site/data/html dir/2018/05/23/2018052300694.html
url<-http:news.chosun.com/site/data/html dir/2018/05/23/2018052300694.html
url<-http://news.chosun.com/site/data/html dir/2018/05/23/2018052300694.html
url<-"http://news.chosun.com/site/data/html dir/2018/05/23/2018052300694.html"
page<-getURL(url,encoding="euc-kr")
#install.packages("RCurl")
library(RCurl)
url<-http://news.chosun.com/site/data/html dir/2018/05/23/2018052300694.html
url<-"http://news.chosun.com/site/data/html dir/2018/05/23/2018052300694.html"
page<-getURL(url,encoding="euc-kr")
download.file(url,"D:\BigDataCampus\Rcrawl\page.txt")
download.file(url,"D:/BigDataCampus/Rcrawl/page.txt")
download.file(url,"D:\BigDataCampus\Rcrawl\page.txt")
download.file(url,"D:\BigDataCampus\Rcrawl")
download.file(url,"D:/BigDataCampus/Rcrawl")
download.file(url,"D:/BigDataCampus/Rcrawl/page.txt")
url<-"http://news.chosun.com/site/data/html dir/2018/05/23/2018052300694.html"
page<-getURL(url,encoding="euc-kr")
download.file(url,"D:/BigDataCampus/Rcrawl/page.txt")
url<-"http://news.chosun.com/site/data/html_dir/2018/05/23/2018052300694.html"
page=getURL(url,.encoding = "euc-kr")
download.file(url,"page.txt")
url<-"http://news.chosun.com/site/data/html_dir/2018/05/23/2018052300694.html"
page<-getURL(url,encoding="euc-kr")
download.file(url,"D:/BigDataCampus/Rcrawl/page.txt")
