print("Hi function!")
sum<-function(x,y){
print("Hi function!")
return (x+y)
}
sum(2,5)
sum(x,y){
print("Hi function!")
return (x+y)
}
sum<-function(x,y){
print("Hi function!")
return (x+y)
}
sum(2,5)
return print("Hi end")
return print("Hi end")
print("Hi end")
return (x+y)
sum<-function(x,y){
print("Hi function!")
if (x>y)
print("Hi end")
return (x+y)
}
sum(2,5)
print("Hi end")
if (x>y){
print("Hi end")
}
return (x+y)
sum<-function(x,y){
print("Hi function!")
if (x>y){
print("Hi end")
}
return (x+y)
}
sum(2,5)
print("Hi function!")
return (x+y)
sum<-function(x,y){
print("Hi function!")
return (x+y)
}
sum(2,5)
print("I want to go My Home.. plz")
return (x+y)
sum<-function(x,y){
print("Hi function!")
print("I want to go My Home.. plz")
return (x+y)
}
sum(2,5)
?if
sum(2,5)
sum(2,5)
sum(2,5)
getwd()
read.excel("Order1501.xlsx",sep=",",header=T,stringAsfactor=F)
read.csv("Order1501.xlsx",sep=",",header=T,stringAsfactor=F)
read.table("Order1501.xlsx",sep=",",header=T,stringAsfactor=F)
read.csv("Order1501.csv",sep=",",header=T,stringAsfactor=F)
read.csv("Order1501.csv",sep=",",header=T,stringAsfactor=F)
?read.csv
read.csv("Order1501.csv",sep=",",header=T,stringAsFactor=F)
read.csv("Order1501.csv",sep=",",header=T,stringsAsFactors = F)
DATA=read.csv("Order1501.csv",sep=",",header=T,stringsAsFactors = F)
DATA
str(DATA)
library(dplyr)
DATA[,-c(12,13,16)]
DATA[,-c(12,13,16)]
DATA=DATA[,-c(12,13,16)]
DATA
DATA %>% filter(ItemType=="TAN")
# ItemType은 filter조건을 사용하여 TAN인 경우의 데이터만 모은다.
# 2단계 과정을 거치면 26만의 데이터에서 16만 데이터 개수로 축소
DATA=DATA %>% filter(ItemType=="TAN")
DATA
# 3. Cancel Code에서 결측치가 있는 레코드가 주요한 데이터이다.
DATA %>% filter(is.na(CancelCode))
# 3. Cancel Code에서 결측치가 있는 레코드가 주요한 데이터이다.
DATA=DATA %>% filter(is.na(CancelCode))
DATA
DATA=read.csv("Order1501.csv",sep=",",header=T,stringsAsFactors = F)
str(DATA)
# data.frame의 형태이고, 총 266298개의 행을 가지며 17개의 독립변수가 존재
# 1. dealerpriceamt, c53amt,code는 고려하지 않는다. 즉, 제거한다.
# 해당 column을 전체 삭제합니다.
DATA=DATA[,-c(12,13,16)]
# 2. itemtype에서 TAN만 고려한다.
# ItemType은 filter조건을 사용하여 TAN인 경우의 데이터만 모은다.
# 2단계 과정을 거치면 26만의 데이터에서 16만 데이터 개수로 축소
DATA=DATA %>% filter(ItemType=="TAN")
DATA
# 3. Cancel Code에서 결측치가 있는 레코드가 주요한 데이터이다.
DATA=DATA %>% filter(is.na(CancelCode))
DATA
DATA=read.csv("Order1501.csv",sep=",",header=T,stringsAsFactors = F)
str(DATA)
# 1. dealerpriceamt, c53amt,code는 고려하지 않는다. 즉, 제거한다.
# 해당 column을 전체 삭제합니다.
DATA=DATA[,-c(12,13,16)]
str(DATA)
# ItemType은 filter조건을 사용하여 TAN인 경우의 데이터만 모은다.
# 2단계 과정을 거치면 26만의 데이터에서 16만 데이터 개수로 축소
DATA=DATA %>% filter(ItemType=="TAN")
DATA
# ItemType은 filter조건을 사용하여 TAN인 경우의 데이터만 모은다.
# 2단계 과정을 거치면 26만의 데이터에서 16만 데이터 개수로 축소
DATA=DATA %>% filter(ItemType=="TAN" & is.na(CancelCode))
DATA
DATA=read.csv("Order1501.csv",sep=",",header=T,stringsAsFactors = F)
str(DATA)
# 1. dealerpriceamt, c53amt,code는 고려하지 않는다. 즉, 제거한다.
# 해당 column을 전체 삭제합니다.
DATA=DATA[,-c(12,13,16)]
str(DATA)
# ItemType은 filter조건을 사용하여 TAN인 경우의 데이터만 모은다.
# 2단계 과정을 거치면 26만의 데이터에서 16만 데이터 개수로 축소
DATA=DATA %>% filter(ItemType=="TAN" & is.na(CancelCode))
str(DATA)
install.packages("rJava")
install.packages("rJava")
install.packages("rJava")
install.packages("rJava")
install.packages("rJava")
install.packages("rJava")
library(RCurl)
library(rvest)
news<-getURL(url,encoding="UTF-8")
url<-"http://news.chosun.com/site/data/html_dir/2018/05/23/2018052300694.html"
news<-getURL(url,encoding="UTF-8")
new1<-html_nodes(news,"h3.news_subtitle")
new1<-html_nodes(news,".par")
new1<-html_nodes(news,"h3.news_subtitle")
url<-"http://news.chosun.com/site/data/html_dir/2019/07/30/2019073002747.html"
news<-read_html(url,encoding="UTF-8")
new1<-html_nodes(news,"h3.news_subtitle")
news2<-html_text(news1,"news_subtitle")
url<-"http://news.chosun.com/site/data/html_dir/2019/07/30/2019073002747.html"
news<-read_html(url,encoding="UTF-8")
news1<-html_nodes(news,"h3.news_subtitle")
news2<-html_text(news1,"news_subtitle")
news2
library(dplyr)
# news1과 news2를 합친 코드 입니다.
news3<-news %>% html_node("h3.news_subtitle") %>% html_test()
# news1과 news2를 합친 코드 입니다.
news3<-news %>% html_node("h3.news_subtitle") %>% html_text()
news3
news3
page<-read_html(url,encoding='UTF-8')
partMain<-page %>% html_node(".par") %>% html_text()
partMain
page<-read_html(url,encoding='UTF-8')
partMain<-page %>% html_nodes(".par") %>% html_text()
partMain
writers<-page %>% html_nodes(".author a") %>% html_text()
writers
url<-"http://v.media.daum.netv20180523174204909?rcmd=rn"
page<-read_html(url,encoding = "UTF-8")
url<-"http://v.media.daum.netv20180523174204909?rcmd=rn"
page<-read_html(url,encoding = "UTF-8")
url<-"http://v.media.daum.net/v/20180523174204909?rcmd=rn"
url<-"http://v.media.daum.net/v/20180523174204909?rcmd=rn"
page<-read_html(url,encoding = "UTF-8")
main<-page %>% html_node("#harmonyContainer") %>% html_text()
main
url<-"http://www.inu.ac.kr/mbshome/mbs/inu/index.do"
page<-getURL(url,encoding="euc-kr")
download.file(url,"D:/BigDataCampus/Rcrawl")
download.file(url,"D:/BigDataCampus")
download.file(url,"D:")
download.file(url,"D:\BigDataCampus")
download.file(url,"D:/BigDataCampus")
download.file(url,"D:/BigDataCampus/incheon.txt")
page<-read_html(url,encoding="UTF-8")
title<-page %>% html_node("title") %>% html_text()
title
url<-"https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=105&oid=277&aid=0004510528"
page<-read_html(url,encoding="UTF-8")
title<-page %>% html_node("title") %>% html_text()
title
url<-"https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=105&oid=277&aid=0004510528"
page<-read_html(url,encoding="UTF-8")
page<-read_html(url,encoding="UTF-8")
page<-read_html(url,encoding="euc-kr")
title<-page %>% html_node("title") %>% html_text()
title
main<-page %>% html_nodes("#articleBodyContents") %>% html_text()
main
write(main,file="Apple_news.txt")
title
main
title
main
library(rJava)
install.packages("rJava")
library(rJava)
library(rJava)
library(rJava)
library(RCurl)
library(rvest)
library(dplyr)
library(xml2)
urlnews<-"https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day&date=20190728"
newsData<-read_html(urlnews)
sectionName<-c("Policy","Economy","Society","Life","World","IT")
#헤더라인 5개를 각 카테로기 별로 입력하는 데이터 프레임을 만듭니다.
newsHead<-data.frame(head_1=c(1:6),head_2=NA,head_3=NA,head_4=NA,head_5=NA,head_6=NA)
rownames(newsHead)<-sectionName
library(rJava)
library(rJava)
library(RCurl)
library(rvest)
library(dplyr)
library(xml2)
urlnews<-"https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day&date=20190728"
newsData<-read_html(urlnews)
# 카테고리 명칭 작성합니다.
sectionName<-c("Policy","Economy","Society","Life","World","IT")
#헤더라인 5개를 각 카테로기 별로 입력하는 데이터 프레임을 만듭니다.
newsHead<-data.frame(head_1=c(1:6),head_2=NA,head_3=NA,head_4=NA,head_5=NA,head_6=NA)
# row name을 카테고리 명칭으로 변경(추출하는 csv에서 첫 번째 열에 나타나도록 합니다.)
rownames(newsHead)<-sectionName
for(i in 1:(ncol(newsHead))){
findNum<-paste()('.num',i)
nodeValue<-html_nodes(newsData,findNum)
for(j in 1:nrow(newsHead)){
extractHead<-gsub("\t","",nodeValue[j] %>% html_text()) %>% strsplit("\n")
trimHead<-trimws(extractHead[[1]])
selectHead<-trimHead[-which(trimHead=="")]
if(selectHead[]=="동영상기사"){
newsHead[j,i]<-selectHead[2]
else{
newsHead[j,i]<-selectHead[1]
}
}
}
}
for(i in 1:(ncol(newsHead))){
findNum<-paste()('.num',i)
nodeValue<-html_nodes(newsData,findNum)
for(j in 1:nrow(newsHead)){
extractHead<-gsub("\t","",nodeValue[j] %>% html_text()) %>% strsplit("\n")
trimHead<-trimws(extractHead[[1]])
selectHead<-trimHead[-which(trimHead=="")]
if(selectHead[]=="동영상기사"){
newsHead[j,i]<-selectHead[2]
else{
newsHead[j,i]<-selectHead[1]
}
}
}
}
urlnews<-"https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day&date=20190728"
newsData<-read_html(urlnews)
# 카테고리 명칭 작성합니다.
secion.names=c("Politics","Economy","Society","Life","World","IT")
#헤더라인 5개를 각 카테로기 별로 입력하는 데이터 프레임을 만듭니다.
news.head=data.frame(head_1=c(1:6),head_2=NA,head_3=NA,head_4=NA,head_5=NA)
# row name을 카테고리 명칭으로 변경(추출하는 csv에서 첫 번째 열에 나타나도록 합니다.)
rownames(news.head)=secion.names
for(i in 1:ncol(news.head)){
findNum=paste0(".num",i)
nodeValue=html_nodes(news,findNum)
for(j in 1:nrow(news.head)){
extracted.head=gsub("\t","",nodeValue[j]%>%html_text())%>%strsplit("\n")
trim.head=trimws(extracted.head[[1]])
selected.head=trim.head[-which(trim.head=="")]
if(selected.head[1]=="동영상기사"){
news.head[j,i]=selected.head[2]
}
else{
news.head[j,i]=selected.head[1]
}
}
}
urlnews<-"https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day&date=20190728"
newsData<-read_html(urlnews)
# 카테고리 명칭 작성합니다.
secion.names=c("Politics","Economy","Society","Life","World","IT")
#헤더라인 5개를 각 카테로기 별로 입력하는 데이터 프레임을 만듭니다.
news.head=data.frame(head_1=c(1:6),head_2=NA,head_3=NA,head_4=NA,head_5=NA)
# row name을 카테고리 명칭으로 변경(추출하는 csv에서 첫 번째 열에 나타나도록 합니다.)
rownames(news.head)=secion.names
for(i in 1:ncol(news.head)){
findNum=paste0(".num",i)
nodeValue=html_nodes(news,findNum)
for(j in 1:nrow(news.head)){
extracted.head=gsub("\t","",nodeValue[j]%>%html_text())%>%strsplit("\n")
trim.head=trimws(extracted.head[[1]])
selected.head=trim.head[-which(trim.head=="")]
if(selected.head[1]=="동영상기사"){
news.head[j,i]=selected.head[2]
}
else{
news.head[j,i]=selected.head[1]
}
}
}
url="https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day&date=20190728"
news=read_html(url)
# 카테고리 명칭 작성합니다.
secion.names=c("Politics","Economy","Society","Life","World","IT")
#헤더라인 5개를 각 카테로기 별로 입력하는 데이터 프레임을 만듭니다.
news.head=data.frame(head_1=c(1:6),head_2=NA,head_3=NA,head_4=NA,head_5=NA)
# row name을 카테고리 명칭으로 변경(추출하는 csv에서 첫 번째 열에 나타나도록 합니다.)
rownames(news.head)=secion.names
for(i in 1:ncol(news.head)){
findNum=paste0(".num",i)
nodeValue=html_nodes(news,findNum)
for(j in 1:nrow(news.head)){
extracted.head=gsub("\t","",nodeValue[j]%>%html_text())%>%strsplit("\n")
trim.head=trimws(extracted.head[[1]])
selected.head=trim.head[-which(trim.head=="")]
if(selected.head[1]=="동영상기사"){
news.head[j,i]=selected.head[2]
}
else{
news.head[j,i]=selected.head[1]
}
}
}
write.csv(newsHead,"popularNewsHeadlines.csv")
newsHead
write.csv(newsHead,"popularNewsHeadlines.csv",row.names=T)
newsHead
news.Head
newsHead
url="https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day&date=20190728"
news=read_html(url)
# 카테고리 명칭 작성합니다.
secion.names=c("Politics","Economy","Society","Life","World","IT")
#헤더라인 5개를 각 카테로기 별로 입력하는 데이터 프레임을 만듭니다.
news.head=data.frame(head_1=c(1:6),head_2=NA,head_3=NA,head_4=NA,head_5=NA)
# row name을 카테고리 명칭으로 변경(추출하는 csv에서 첫 번째 열에 나타나도록 합니다.)
rownames(news.head)=secion.names
for(i in 1:ncol(news.head)){ # 현재 6행이므로 6번 반복
findNum=paste0(".num",i)  # 헤드라인마다 번호가 다르므로 번호를 바꿀 수 있도록 합니다.
nodeValue=html_nodes(news,findNum) #class가 6개 이므로 node 6개여야 합니다.
for(j in 1:nrow(news.head)){  # 정치~it 까지의 카테고리에 대한 for 문
# gsub()로 데이터표현을 바꿉니다. "\t" -> "" 로 바꿉니다.
extracted.head=gsub("\t","",nodeValue[j]%>%html_text())%>%strsplit("\n")
trim.head=trimws(extracted.head[[1]])   # 글자 앞뒤의 빈공간을 엾애줍니다.
selected.head=trim.head[-which(trim.head=="")]   # 공란으로 된 character를 제외한 나머지를 찾습니다.
if(selected.head[1]=="동영상기사"){  # 가끔 동영상 기사의 경우, 헤드보다 앞에 표시가 되도록 합니다.
news.head[j,i]=selected.head[2]
}
else{
news.head[j,i]=selected.head[1]
}
}
}
news.head
write.csv(news.head,"popularNewsHeadlines.csv",row.names=T)
news.head
library(lubridate)
setDate<-ymd("2019-07-30")-days(0:365)
setDate<-format(setDate,"%Y%m%d")
setDate
setDate<-ymd("2019-07-30")-days(0:365)
setDate<-format(setDate,"%Y%m%d")
for(d in 1:length(setDate)){
urlNews<-paste0("https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day&date=",setDate[d])
newsData<-read_html(urlNews)
sectionName<-c("Policy","Economy","Society","Life","World","It")
newsHead=data.frame(head_1=c(1:6),head_2=NA,head_3=NA,head_4=NA,head_5=NA)
rownames(newsHead)<-sectionName
for(i in 1:(ncol(newsHead))){
findNum<-paste0(".num",i)
nodeValue<-html_nodes(newsData,findNum)
for(j in 1:nrow(newsHead)){
extractHead<-gsub("\t","",nodeValue[j] %>% html_text()) %>% strsplit("\n")
trimHead<-trimws(extractHead[[1]])
selectHead<-trimHead[-which(trimHead=="")]
if(selectHead[1]=="동영상기사"){
newsHead[j,i]<-selectHead[2]
}
else{
newsHead[j,i]<-selectHead[1]
}
}
}
write.csv(newsHead,paste0("poopularNewsHeadlines(",setDate[d],").csv"))
}
page<-read_html(url,encoding='euc-kr')
url<-"https://finance.naver.com/item/board.nhn?code=005930&page=1"
page<-read_html(url,encoding='euc-kr')
page2<-html_nodes(page,'td.title a')
page3<-html_attr(page2,'title')
page4<-as.data.frame(page3)
page4
url<-"https://finance.naver.com/item/board.nhn?code=005930&page=1"
page<-read_html(url,encoding='euc-kr')
page2<-html_nodes(page,'td.title a')
page3<-html_attr(page2,'title')
page4<-as.data.frame(page3)
page4
page5<-list()
for(i in 1:10){
url<-paste("https://finance.naver.com/item/board.nhn?code=005930&page=",i,sep='')
page5[[i]]<-read_html(url,encoding='euc-kr') %>% html_nodes("td.title a") %>% html_attr('title')
}
page5
write.csv(unlist(page5),'ddt.csv')
install.packages("KoNLP")
install_github("SukjaeChoi/RHINO")
.libPaths()
#20190802 비정형 데이터 분석
library(RHINO)
library(makeDTM)
library(tm)
setwd("D:/BigDataCampus/practiceData/2차 실습파일")
#setwd("D:/BigDataCampus/practiceData")
# 통상적으로 +-0.3 이상이면 상관관계가 있음으로 판단한다.
cor(iris$Sepal.Width,iris$Sepal.Length)   # -0.1175698
cor(iris[,1:4])
iris.cor<-cor(iris[,1:4])
# 전체적인 상관관계를 기호로 표시해서 보여준다. symnum()
symnum(iris.cor)
initRhino()
# 텍스트를 읽어옵니다.
txt<-readLines("sample_news.txt")
# 텍스트에서 명사만 뽑아옵니다.
noun<-lapply(txt,getMorph,"noun")
# 리스트에서 벡터형태로 형변환을 합니다.
nounVec<-unlist(noun)
nounFreq<-table(nounVec)
# 빈도수를 내림차순 정렬하고, 20개 명사를 keywords에 저장합니다.
keywords<-names(head(sort(nounFreq,decreasing = T),20))
docs<-as.data.frame(txt)
# makeDTM() 함수를 이용해서 docs를 DTM형태로 구성합니다.
dtm<-makeDTM(docs,key=keywords,TEXT.name="txt",RHINO=T)
# DTM으로 만든 후, 상관관계를 알고싶은 열만 추출합니다.
word1<-as.vector(dtm[,"정부"])
word2<-as.vector(dtm[,"가계"])
cor(word1,word2) #-0.375
cor.test(word1,word2)
# DTM, 분석자가 지정한 단어, 분석자가 설정한 피어슨 상관계수 크기 기준
findAssocTwo(dtm,"계획","투자")
findAssocsAll(dtm)
library(arules)
library(arulesViz)
data<-read.transactions("groceries.csv",rm.duplicates = T,sep=",")
data
summary(data)
itemFrequencyPlot(data,support=0.01,main="Item Frequency Plot support 0.01")
itemFrequencyPlot(data,support=0.1,main="Item Frequency Plot support 0.1")
rules<-apriori(data,parameter=list(support=0.01.confidence=0.8))
rules<-apriori(data,parameter = list(support=0.01,confidence=0.8))
rules<-apriori(data,parameter = list(support=0.001,confidence=0.8))
summary(rules)
inspect(head(sort(rules,by="confidence"),5))
interest<-subset(rules,items %in% c("whole milk","butter"))
inspect(interest[1:5])
txt<-readLines("sample_news.txt")
noun<-lapply(txt,getMorph,"noun")
nounVec<-unlist(noun)
nounFreq<-table(nounVec)
keywords<-names(head(sort(nounFreq,decreasing = T),20))
keywords
docs<-as.data.frame(txt)
dtm<-makeDTM(docs,key=keywords,TEXT.name="txt",RHINO=T)
dtm
dtm.df<-as.data.frame(dtm)
dtm.df
dtm.abovemean<-ifelse(dtm>mean(apply(dtm,2,mean)),1,0)
dtm.abovemean
rules<-apriori(dtm.abovemean,parameter=list(support=0.3,conf=0.8))
arules::inspect(sort(rules))
rules2<-subset(rules,subset=lhs %pin% "경재" & confidence>0.7)
inspect(sort(rules2))
rules2<-subset(rules,subset=lhs %pin% "경재" & confidence>0.7)
inspect(sort(rules2))
rules2
rules2<-subset(rules,subset=lhs %pin% "경재" & confidence>0.7)
inspect(sort(rules2))
plot(rules,method="graph",nodeCol="blue")
rules.2<-labels(rules,ruleSep=" ")
rules.3<-sapply(rules.2,strsplit," ",USE.NAMES=F)
rule.mat<-do.call("rbind",rules.3)
rule.mat
install.packages("igraph")
install.packages("igraph")
library(igraph)
rule.g<-graph.edgelist(rule.mat,directed=T)
rule.g
plot.igraph(rule.g)
plot.igraph(rule.g,vertex.label=V(rule.g)$name,vertex.label.cex=1.1,vertex.label.color="black",vertex.size=20,
vertex.color="green",vertex.frame,color="blue",edge.arrow.size=0.5,edge.arrow.width=0.7)
plot.igraph(rule.g)
install.packages("corrplot")
library(corrplot)
corrplot(cor(dtm.abovemean))
txt<-readLines("sample_news.txt")
noun<-lapply(txt,getMorph,"noun")
noun<-lapply(txt,getMorph,"noun")
noun
dtm
install.packages("topicmodels")
#install.packages("topicmodels")
library(topicmodels)
lda<-LDA(dtm,k=5)
terms(lda,10)
library(e1071)
tune.svm(factor(y_faulty)~.,data=autoparts2,gamma=2^(-1:1),cost = 2^(2:4))
docs<-read.csv("보험_sample_train.csv")
data.cluster<-kmeans(dtm,centers=3)
data.cluster
