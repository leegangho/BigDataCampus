DATA
DATA=read.csv("Order1501.csv",sep=",",header=T,stringsAsFactors = F)
str(DATA)
# data.frame의 형태이고, 총 266298개의 행을 가지며 17개의 독립변수가 존재
# 1. dealerpriceamt, c53amt,code는 고려하지 않는다. 즉, 제거한다.
# 해당 column을 전체 삭제합니다.
DATA=DATA[,-c(12,13,16)]
# 2. itemtype에서 TAN만 고려한다.
# ItemType은 filter조건을 사용하여 TAN인 경우의 데이터만 모은다.
# 2단계 과정을 거치면 26만의 데이터에서 16만 데이터 개수로 축소
DATA=DATA %>% filter(ItemType=="TAN")
DATA
# 3. Cancel Code에서 결측치가 있는 레코드가 주요한 데이터이다.
DATA=DATA %>% filter(is.na(CancelCode))
DATA
DATA=read.csv("Order1501.csv",sep=",",header=T,stringsAsFactors = F)
str(DATA)
# 1. dealerpriceamt, c53amt,code는 고려하지 않는다. 즉, 제거한다.
# 해당 column을 전체 삭제합니다.
DATA=DATA[,-c(12,13,16)]
str(DATA)
# ItemType은 filter조건을 사용하여 TAN인 경우의 데이터만 모은다.
# 2단계 과정을 거치면 26만의 데이터에서 16만 데이터 개수로 축소
DATA=DATA %>% filter(ItemType=="TAN")
DATA
# ItemType은 filter조건을 사용하여 TAN인 경우의 데이터만 모은다.
# 2단계 과정을 거치면 26만의 데이터에서 16만 데이터 개수로 축소
DATA=DATA %>% filter(ItemType=="TAN" & is.na(CancelCode))
DATA
DATA=read.csv("Order1501.csv",sep=",",header=T,stringsAsFactors = F)
str(DATA)
# 1. dealerpriceamt, c53amt,code는 고려하지 않는다. 즉, 제거한다.
# 해당 column을 전체 삭제합니다.
DATA=DATA[,-c(12,13,16)]
str(DATA)
# ItemType은 filter조건을 사용하여 TAN인 경우의 데이터만 모은다.
# 2단계 과정을 거치면 26만의 데이터에서 16만 데이터 개수로 축소
DATA=DATA %>% filter(ItemType=="TAN" & is.na(CancelCode))
str(DATA)
install.packages("rJava")
install.packages("rJava")
install.packages("rJava")
install.packages("rJava")
install.packages("rJava")
install.packages("rJava")
library(RCurl)
library(rvest)
news<-getURL(url,encoding="UTF-8")
url<-"http://news.chosun.com/site/data/html_dir/2018/05/23/2018052300694.html"
news<-getURL(url,encoding="UTF-8")
new1<-html_nodes(news,"h3.news_subtitle")
new1<-html_nodes(news,".par")
new1<-html_nodes(news,"h3.news_subtitle")
url<-"http://news.chosun.com/site/data/html_dir/2019/07/30/2019073002747.html"
news<-read_html(url,encoding="UTF-8")
new1<-html_nodes(news,"h3.news_subtitle")
news2<-html_text(news1,"news_subtitle")
url<-"http://news.chosun.com/site/data/html_dir/2019/07/30/2019073002747.html"
news<-read_html(url,encoding="UTF-8")
news1<-html_nodes(news,"h3.news_subtitle")
news2<-html_text(news1,"news_subtitle")
news2
library(dplyr)
# news1과 news2를 합친 코드 입니다.
news3<-news %>% html_node("h3.news_subtitle") %>% html_test()
# news1과 news2를 합친 코드 입니다.
news3<-news %>% html_node("h3.news_subtitle") %>% html_text()
news3
news3
page<-read_html(url,encoding='UTF-8')
partMain<-page %>% html_node(".par") %>% html_text()
partMain
page<-read_html(url,encoding='UTF-8')
partMain<-page %>% html_nodes(".par") %>% html_text()
partMain
writers<-page %>% html_nodes(".author a") %>% html_text()
writers
url<-"http://v.media.daum.netv20180523174204909?rcmd=rn"
page<-read_html(url,encoding = "UTF-8")
url<-"http://v.media.daum.netv20180523174204909?rcmd=rn"
page<-read_html(url,encoding = "UTF-8")
url<-"http://v.media.daum.net/v/20180523174204909?rcmd=rn"
url<-"http://v.media.daum.net/v/20180523174204909?rcmd=rn"
page<-read_html(url,encoding = "UTF-8")
main<-page %>% html_node("#harmonyContainer") %>% html_text()
main
url<-"http://www.inu.ac.kr/mbshome/mbs/inu/index.do"
page<-getURL(url,encoding="euc-kr")
download.file(url,"D:/BigDataCampus/Rcrawl")
download.file(url,"D:/BigDataCampus")
download.file(url,"D:")
download.file(url,"D:\BigDataCampus")
download.file(url,"D:/BigDataCampus")
download.file(url,"D:/BigDataCampus/incheon.txt")
page<-read_html(url,encoding="UTF-8")
title<-page %>% html_node("title") %>% html_text()
title
url<-"https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=105&oid=277&aid=0004510528"
page<-read_html(url,encoding="UTF-8")
title<-page %>% html_node("title") %>% html_text()
title
url<-"https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=105&oid=277&aid=0004510528"
page<-read_html(url,encoding="UTF-8")
page<-read_html(url,encoding="UTF-8")
page<-read_html(url,encoding="euc-kr")
title<-page %>% html_node("title") %>% html_text()
title
main<-page %>% html_nodes("#articleBodyContents") %>% html_text()
main
write(main,file="Apple_news.txt")
title
main
title
main
library(rJava)
install.packages("rJava")
library(rJava)
library(rJava)
library(rJava)
library(RCurl)
library(rvest)
library(dplyr)
library(xml2)
urlnews<-"https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day&date=20190728"
newsData<-read_html(urlnews)
sectionName<-c("Policy","Economy","Society","Life","World","IT")
#헤더라인 5개를 각 카테로기 별로 입력하는 데이터 프레임을 만듭니다.
newsHead<-data.frame(head_1=c(1:6),head_2=NA,head_3=NA,head_4=NA,head_5=NA,head_6=NA)
rownames(newsHead)<-sectionName
library(rJava)
library(rJava)
library(RCurl)
library(rvest)
library(dplyr)
library(xml2)
urlnews<-"https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day&date=20190728"
newsData<-read_html(urlnews)
# 카테고리 명칭 작성합니다.
sectionName<-c("Policy","Economy","Society","Life","World","IT")
#헤더라인 5개를 각 카테로기 별로 입력하는 데이터 프레임을 만듭니다.
newsHead<-data.frame(head_1=c(1:6),head_2=NA,head_3=NA,head_4=NA,head_5=NA,head_6=NA)
# row name을 카테고리 명칭으로 변경(추출하는 csv에서 첫 번째 열에 나타나도록 합니다.)
rownames(newsHead)<-sectionName
for(i in 1:(ncol(newsHead))){
findNum<-paste()('.num',i)
nodeValue<-html_nodes(newsData,findNum)
for(j in 1:nrow(newsHead)){
extractHead<-gsub("\t","",nodeValue[j] %>% html_text()) %>% strsplit("\n")
trimHead<-trimws(extractHead[[1]])
selectHead<-trimHead[-which(trimHead=="")]
if(selectHead[]=="동영상기사"){
newsHead[j,i]<-selectHead[2]
else{
newsHead[j,i]<-selectHead[1]
}
}
}
}
for(i in 1:(ncol(newsHead))){
findNum<-paste()('.num',i)
nodeValue<-html_nodes(newsData,findNum)
for(j in 1:nrow(newsHead)){
extractHead<-gsub("\t","",nodeValue[j] %>% html_text()) %>% strsplit("\n")
trimHead<-trimws(extractHead[[1]])
selectHead<-trimHead[-which(trimHead=="")]
if(selectHead[]=="동영상기사"){
newsHead[j,i]<-selectHead[2]
else{
newsHead[j,i]<-selectHead[1]
}
}
}
}
urlnews<-"https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day&date=20190728"
newsData<-read_html(urlnews)
# 카테고리 명칭 작성합니다.
secion.names=c("Politics","Economy","Society","Life","World","IT")
#헤더라인 5개를 각 카테로기 별로 입력하는 데이터 프레임을 만듭니다.
news.head=data.frame(head_1=c(1:6),head_2=NA,head_3=NA,head_4=NA,head_5=NA)
# row name을 카테고리 명칭으로 변경(추출하는 csv에서 첫 번째 열에 나타나도록 합니다.)
rownames(news.head)=secion.names
for(i in 1:ncol(news.head)){
findNum=paste0(".num",i)
nodeValue=html_nodes(news,findNum)
for(j in 1:nrow(news.head)){
extracted.head=gsub("\t","",nodeValue[j]%>%html_text())%>%strsplit("\n")
trim.head=trimws(extracted.head[[1]])
selected.head=trim.head[-which(trim.head=="")]
if(selected.head[1]=="동영상기사"){
news.head[j,i]=selected.head[2]
}
else{
news.head[j,i]=selected.head[1]
}
}
}
urlnews<-"https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day&date=20190728"
newsData<-read_html(urlnews)
# 카테고리 명칭 작성합니다.
secion.names=c("Politics","Economy","Society","Life","World","IT")
#헤더라인 5개를 각 카테로기 별로 입력하는 데이터 프레임을 만듭니다.
news.head=data.frame(head_1=c(1:6),head_2=NA,head_3=NA,head_4=NA,head_5=NA)
# row name을 카테고리 명칭으로 변경(추출하는 csv에서 첫 번째 열에 나타나도록 합니다.)
rownames(news.head)=secion.names
for(i in 1:ncol(news.head)){
findNum=paste0(".num",i)
nodeValue=html_nodes(news,findNum)
for(j in 1:nrow(news.head)){
extracted.head=gsub("\t","",nodeValue[j]%>%html_text())%>%strsplit("\n")
trim.head=trimws(extracted.head[[1]])
selected.head=trim.head[-which(trim.head=="")]
if(selected.head[1]=="동영상기사"){
news.head[j,i]=selected.head[2]
}
else{
news.head[j,i]=selected.head[1]
}
}
}
url="https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day&date=20190728"
news=read_html(url)
# 카테고리 명칭 작성합니다.
secion.names=c("Politics","Economy","Society","Life","World","IT")
#헤더라인 5개를 각 카테로기 별로 입력하는 데이터 프레임을 만듭니다.
news.head=data.frame(head_1=c(1:6),head_2=NA,head_3=NA,head_4=NA,head_5=NA)
# row name을 카테고리 명칭으로 변경(추출하는 csv에서 첫 번째 열에 나타나도록 합니다.)
rownames(news.head)=secion.names
for(i in 1:ncol(news.head)){
findNum=paste0(".num",i)
nodeValue=html_nodes(news,findNum)
for(j in 1:nrow(news.head)){
extracted.head=gsub("\t","",nodeValue[j]%>%html_text())%>%strsplit("\n")
trim.head=trimws(extracted.head[[1]])
selected.head=trim.head[-which(trim.head=="")]
if(selected.head[1]=="동영상기사"){
news.head[j,i]=selected.head[2]
}
else{
news.head[j,i]=selected.head[1]
}
}
}
write.csv(newsHead,"popularNewsHeadlines.csv")
newsHead
write.csv(newsHead,"popularNewsHeadlines.csv",row.names=T)
newsHead
news.Head
newsHead
url="https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day&date=20190728"
news=read_html(url)
# 카테고리 명칭 작성합니다.
secion.names=c("Politics","Economy","Society","Life","World","IT")
#헤더라인 5개를 각 카테로기 별로 입력하는 데이터 프레임을 만듭니다.
news.head=data.frame(head_1=c(1:6),head_2=NA,head_3=NA,head_4=NA,head_5=NA)
# row name을 카테고리 명칭으로 변경(추출하는 csv에서 첫 번째 열에 나타나도록 합니다.)
rownames(news.head)=secion.names
for(i in 1:ncol(news.head)){ # 현재 6행이므로 6번 반복
findNum=paste0(".num",i)  # 헤드라인마다 번호가 다르므로 번호를 바꿀 수 있도록 합니다.
nodeValue=html_nodes(news,findNum) #class가 6개 이므로 node 6개여야 합니다.
for(j in 1:nrow(news.head)){  # 정치~it 까지의 카테고리에 대한 for 문
# gsub()로 데이터표현을 바꿉니다. "\t" -> "" 로 바꿉니다.
extracted.head=gsub("\t","",nodeValue[j]%>%html_text())%>%strsplit("\n")
trim.head=trimws(extracted.head[[1]])   # 글자 앞뒤의 빈공간을 엾애줍니다.
selected.head=trim.head[-which(trim.head=="")]   # 공란으로 된 character를 제외한 나머지를 찾습니다.
if(selected.head[1]=="동영상기사"){  # 가끔 동영상 기사의 경우, 헤드보다 앞에 표시가 되도록 합니다.
news.head[j,i]=selected.head[2]
}
else{
news.head[j,i]=selected.head[1]
}
}
}
news.head
write.csv(news.head,"popularNewsHeadlines.csv",row.names=T)
news.head
library(lubridate)
setDate<-ymd("2019-07-30")-days(0:365)
setDate<-format(setDate,"%Y%m%d")
setDate
setDate<-ymd("2019-07-30")-days(0:365)
setDate<-format(setDate,"%Y%m%d")
for(d in 1:length(setDate)){
urlNews<-paste0("https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day&date=",setDate[d])
newsData<-read_html(urlNews)
sectionName<-c("Policy","Economy","Society","Life","World","It")
newsHead=data.frame(head_1=c(1:6),head_2=NA,head_3=NA,head_4=NA,head_5=NA)
rownames(newsHead)<-sectionName
for(i in 1:(ncol(newsHead))){
findNum<-paste0(".num",i)
nodeValue<-html_nodes(newsData,findNum)
for(j in 1:nrow(newsHead)){
extractHead<-gsub("\t","",nodeValue[j] %>% html_text()) %>% strsplit("\n")
trimHead<-trimws(extractHead[[1]])
selectHead<-trimHead[-which(trimHead=="")]
if(selectHead[1]=="동영상기사"){
newsHead[j,i]<-selectHead[2]
}
else{
newsHead[j,i]<-selectHead[1]
}
}
}
write.csv(newsHead,paste0("poopularNewsHeadlines(",setDate[d],").csv"))
}
page<-read_html(url,encoding='euc-kr')
url<-"https://finance.naver.com/item/board.nhn?code=005930&page=1"
page<-read_html(url,encoding='euc-kr')
page2<-html_nodes(page,'td.title a')
page3<-html_attr(page2,'title')
page4<-as.data.frame(page3)
page4
url<-"https://finance.naver.com/item/board.nhn?code=005930&page=1"
page<-read_html(url,encoding='euc-kr')
page2<-html_nodes(page,'td.title a')
page3<-html_attr(page2,'title')
page4<-as.data.frame(page3)
page4
page5<-list()
for(i in 1:10){
url<-paste("https://finance.naver.com/item/board.nhn?code=005930&page=",i,sep='')
page5[[i]]<-read_html(url,encoding='euc-kr') %>% html_nodes("td.title a") %>% html_attr('title')
}
page5
write.csv(unlist(page5),'ddt.csv')
install.packages("KoNLP")
install_github("SukjaeChoi/RHINO")
.libPaths()
x <- c(1,2,2,1,3,3,1,5)
x2 <- table(x)
library(RColorBrewer)
bp <- barplot(x2)
percent <- round(x2/sum(x2)*100,digits=1)
bp <- barplot(x2)
percent <- round(x2/sum(x2)*100,digits=1)
text(x=bp,y=x2-0.1,labels=percent)
pal <- brewer.pal(12,"Set3")
ｂｐ <- ｂａｒｐｌｏｔ（ｘ２，ｃｏｌ＝ｐａｌ）
setwd("D:/BigDataCampus/practiceData/file")
data <- read.csv("hit.csv",header=TRUE)
avg<-data$average
name<-data$name
ggplot(data,aes(x=avg,y=name))+geom_point()
#20190807
library(ggplot)
#20190807
library(ggplot2)
setwd("D:/BigDataCampus/practiceData/file")
data <- read.csv("hit.csv",header=TRUE)
avg<-data$average
name<-data$name
ggplot(data,aes(x=avg,y=name))+geom_point()
ggplot(data,aes(x=avg,y=reorder(name,avg)))+geom_point(size=3)
ggplot(data,aes(x=avg,y=reorder(name,avh)))+geom_point(size=3)+geom_segment(aes(yend-name),xend=0)
ggplot(data,aes(x=avg,y=reorder(name,avg)))+geom_point(size=3)+geom_segment(aes(yend=name),xend=0)
#20190807
library(ggplot2)
setwd("D:/BigDataCampus/practiceData/file")
data <- read.csv("hit.csv",header=TRUE)
avg<-data$average
name<-data$name
ggplot(data,aes(x=avg,y=name))+geom_point()
ggplot(data,aes(x=avg,y=reorder(name,avg)))+geom_point(size=3)
ggplot(data,aes(x=avg,y=reorder(name,avg)))+geom_point(size=3)+geom_segment(aes(yend=name),xend=0)
ggplot(data,aes(x=avg,y=reorder(name,avg)))+geom_point(size=3)+geom_segment(aes(yend=name),xend=0)+ylab("name")
ggplot((data,aes(x=avg,y=reorder(name,avg)))+geom_point(size=3)+geom_segment(aes(yend=name),xend=0)+ylab("name")
#연습
data<-read.csv("hit2.csv",header=TRUE)
avg<-data.$Avg
#연습
data<-read.csv("hit2.csv",header=TRUE)
avg<-data.$Avg
fire<-read.csv("fire.csv")
head(fire)
fire<-read.csv("fire.csv",header=T)
head(fire)
fire<-read.csv("fire.csv")
head(fire)
lat<-fire$LAT
lon<-fire$LON
y2011<-fire$y2011
data<-data.frame(lat,lon,y2011)
data
install.packages("ggmap")
library(ggmap)
map<-get_map("Seoul",zoom=10,maptype = "roadmap")
register_google(key="AIzaSyDb6CtknFf0WsNEHDErgOZZM_pTPWMfPbs")
map<-get_map("Seoul",zoom=10,maptype = "roadmap")
map
map<-ggmap(map)
map
map+geom_point(aes(x=lon,y=lat,color=y2011,size=y2011),data=data)
map
fire<-read.csv("fire.csv")
lat<-fire$LAT
lon<-fire$LON
y2011<-fire$y2011
data<-data.frame(lat,lon,y2011)
map<-get_map("Seoul",zoom=10,maptype = "roadmap")
map<-ggmap(map)
map+geom_point(aes(x=lon,y=lat,color=y2011,size=y2011),data=data)
map
map
map+geom_point(aes(x=lon,y=lat,color=y2011,size=y2011),data=data)
ge<-geocode(enc2utf8("용인"))
cen<-as.numeric(gc)
head(ge)
ge
cen<-as.numeric(ge)
Map <- get_googlemap(center=cen)
ggmap(map)
ggmap(Map)
gc<-geocode(enc2utf8("설악산"))
cen<-as.numeric(gc)
Map<-get_googlemap(center=cen,zoom=8,size=c(640,480),maptype = "hybrid")
ggmap(Map)
ggmap(Map)
Map<-get_googlemap(center=cen,zoom=8,size=c(640,640),maptype = "hybrid")
ggmap(Map)
cen <- c(-118.233248,34.085015)
map<-get_googlemap(center=cen)
Ggmap(map)
ggmap(map)
names<-c("용두암","성산일출봉","정방폭포","중문관광단지","한라산 1100고지","차귀도")
names<-c("용두암","성산일출봉","정방폭포","중문관광단지","한라산 1100고지","차귀도")
addr<-c("제주시 용두암길 15","서귀포시 성산읍 성산리","서귀포시 동홍동 299-3","서귀포시 중문동 2624-1","서귀포시 색달동 산1-2","제주시 한경면 고산리 125")
gc<-geocode(enc2utf8(addr))
af<-data.frame(name=names,lon=gc$lon,lat=gc$lat)
head(af)
cen<-c(mean(af$lon),mean(af$lat))
map<-get_googlemap(center=cen,maptype="roadmap",zoom=10,size=c(640,640),marker=gc)
gmap<-ggmap(map)
gmap+geom_text(data=af,aes(x=lon,y=lat),size=5,label=af$name)
quakes
data("quakes")
head(quakes)
df <- head(quakes,100)
cen <- c(mean(df$long),mean(df$lat))
gc<-data.frame(lon=df$long,lat=df$lat)
# 구글맵에 사용하는 경도 값으로 변환합니다.
gc$lon<-ifelse(gc$lon > 180, -(360-gc$lon),gc$lon)
map <- get_googlemap(center=cen,maptype="roadmap",zoom=4,marker=gc)
ggmap(map)+theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank(),
axis.title.y=element_blank(),
axis.text.y=element_blank(),
axis.ticks.y=element_blank())
mat<-get_googlemap(center=cen,maptype = "roadmap",zoom=5)
gmap<-ggmap(map)
gmap+geom_point(data=df,aes(x=long,y=lat,size=map),alpha=0.5)
gmap+geom_point(data=df,aes(x=long,y=lat,size=mag),alpha=0.5)
df <- head(quakes,100)
cen <- c(mean(df$long),mean(df$lat))
gc<-data.frame(lon=df$long,lat=df$lat)
# 구글맵에 사용하는 경도 값으로 변환합니다.
gc$lon<-ifelse(gc$lon > 180, -(360-gc$lon),gc$lon)
map <- get_googlemap(center=cen,maptype="roadmap",zoom=4,marker=gc)
ggmap(map)+theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank(),
axis.title.y=element_blank(),
axis.text.y=element_blank(),
axis.ticks.y=element_blank())
mat<-get_googlemap(center=cen,maptype = "roadmap",zoom=5)
gmap<-ggmap(map)
gmap+geom_point(data=df,aes(x=long,y=lat,size=mag),alpha=0.5)
gmap
gmap+geom_point(data=df,aes(x=long,y=lat,size=mag),alpha=0.5)
map<-get_googlemap(center=cen,maptype = "roadmap",zoom=5)
gmap<-ggmap(map)
gmap+geom_point(data=df,aes(x=long,y=lat,size=mag),alpha=0.5)
library(treemap)
data<-read.csv("국회의원_선거구_유권자수.csv",header=T)
gsum<-aggregate(data[,5],by=list(data$시도),sum)
gsum
treemap(gsum,index=c("Group.1"),vSize="x",vColor="x",type="value",bg.labels="yellow")
library(MASS)
st<-data.frame(state.x77)
head(st)
symbols(st$Income,st$Illiteracy,circles=st$Population,inches=0.4,fg="white",bg="red",lwd=1.5,xlab="Income",ylab="llliteracy",main="US State data")
text(st$Income,st$Illiteracy,rownames(st),cex=0.5,col="blue")
data("airquality")
boxplot(Temp~Month,data=airquality,xlab="Month Number",ylab="Temp",main="Different boxplots for each month",col="orange")
boxplot(Wind~Month,data=airquality,xlab="Month Number",ylab="Wind",main="Different boxplots for each month",col="orange")
boxplot(Temp~Month,data=airquality,xlab="Month Number",ylab="Temp",main="Different boxplots for each month",col="orange")
boxplot(Wind~Month,data=airquality,xlab="Month Number",ylab="Wind",main="Different boxplots for each month",col="yellow")
boxplot(Temp~Month,data=airquality,xlab="Month Number",ylab="Temp",main="Different boxplots for each month",col="orange")
boxplot(Wind~Month,data=airquality,xlab="Month Number",ylab="Wind",main="Different boxplots for each month",col="yellow")
boxplot(Ozone~Month,data=airquality,xlab="Month Number",ylab="Ozone",main="Different boxplots for each month",col="lightgray")
data("HairEyeColor")
str(HairEyeColor)
mosaicplot(~Eye+Hair,data=HairEyeColor,color=c("red","green"),off=1)
st<-data.frame(state.x77)
ggplot(data=st,aes(x=Income,y=Illiteracy))+geom_point()
cnt<-table(mtcars$gear)
gear<-c(3,4,5)
data<-cbind(cnt,gear)
data<-data.frame(data)
ggplot(data,aes(x=gear,y=cnt))+geom_bar(stat="identity",width=0.7,fill="steelblue")
cnt
data
data("airmiles")
str(airmiles)
head(airmiles)
year<-1937:1960
data<-cbind(year,airmiles)
air<-data.frame(data)
ggplot(air,aex(x=year,y=airmiles))+geom_line(color="red",size=1)
ggplot(air,aes(x=year,y=airmiles))+geom_line(color="red",size=1)
data
str(airmiles)
ggplot()+geom_boxplot(data=iris,aes(x=Species,y=Petal.Width,fill=Species))
